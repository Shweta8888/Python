{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Variable Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many styles of randomness in nature and in theory.  Sometimes the randomness can be described as having a distribution, or probability density function, that is a named function.  The most common and famous one is the Gaussian, or Normal distribution.  The uniform distribution is the simplest possible distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a distribution: quick look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get our notebook session set up with a directive and some standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Our standard imports:\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access to many standard distributions:\n",
    "import scipy.stats as ss\n",
    "\n",
    "# Uncomment for low-res display:\n",
    "#plt.rcParams['figure.dpi'] = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two basic ways of visualizing distributions: as probability density functions (PDF) and as cumulative density functions (CDF).  The CDF is just the integral of the PDF; and because the PDF is the derivative of the CDF, it looks noisier when estimated from a sample of modest size.\n",
    "\n",
    "We will start with two standard distributions: normal (Gaussian), and uniform.  \n",
    "\n",
    "We will also start by plotting using the canned black-box `hist()` function.  Then we will back up and break the operation into its component parts to be sure we understand what is going on.\n",
    "\n",
    "The `hist()` function or Axes method has many options and capabilities--probably too many.  We will use only a few of them. To begin, let's plot counts per bin. We will let the function select the bin boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamp = 1000\n",
    "nbins = nsamp // 50\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "yg = np.random.randn(nsamp)\n",
    "yu = np.random.rand(nsamp)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "ax = axs[0, 0]\n",
    "ax.hist(yu, bins=nbins)\n",
    "ax.set_title('Uniform per bin')\n",
    "\n",
    "ax = axs[1, 0]\n",
    "ax.hist(yu, bins=nbins, cumulative=True)\n",
    "ax.set_title('Uniform cumulative')\n",
    "\n",
    "ax = axs[0, 1]\n",
    "ax.hist(yg, bins=nbins)\n",
    "ax.set_title('Normal per bin')\n",
    "\n",
    "ax = axs[1, 1]\n",
    "ax.hist(yg, bins=nbins, cumulative=True)\n",
    "ax.set_title('Normal cumulative');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use a normalization that provides an approximate PDF and CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamp = 1000\n",
    "nbins = nsamp // 50\n",
    "\n",
    "# keyword arguments for all subplots\n",
    "kwargs = dict(bins=nbins, density=True)\n",
    "\n",
    "yg = np.random.randn(nsamp)\n",
    "yu = np.random.rand(nsamp)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "ax = axs[0, 0]\n",
    "ax.hist(yu, **kwargs)\n",
    "ax.set_title('Uniform PDF')\n",
    "\n",
    "ax = axs[1, 0]\n",
    "ax.hist(yu, cumulative=True, **kwargs)\n",
    "ax.set_title('Uniform CDF')\n",
    "\n",
    "ax = axs[0, 1]\n",
    "ax.hist(yg, **kwargs)\n",
    "ax.set_title('Normal PDF')\n",
    "\n",
    "ax = axs[1, 1]\n",
    "ax.hist(yg, cumulative=True, **kwargs)\n",
    "ax.set_title('Normal CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the integral of the PDF over all possible values is 1, so the CDF goes from 0 to 1. The CDF really should be plotted as a line from point to point and starting at (0, 0), not with steps, but unfortunately Matplotlib's `hist` function lacks an option for this.  Below, we will see how to get around this design error.  But first, let's verify a basic principle in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the Normal distribution so central to statistics?  There's a theorem about that!\n",
    "\n",
    "If you take the average of more and more realizations of any distribution, the distribution of those averages approaches a Gaussian distribution.  Let's illustrate this first with the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 1000000\n",
    "navgs = [1, 3, 5, 7]\n",
    "\n",
    "bins=np.linspace(0, 1, num=51)\n",
    "fig, ax = plt.subplots()\n",
    "for navg in navgs:\n",
    "    y = np.random.rand(npts, navg).mean(axis=-1)\n",
    "    ax.hist(y, histtype='step', bins=bins, density=True, label=str(navg))\n",
    "ax.legend(loc='upper right')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we square the values of a uniform distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 100000\n",
    "navgs = [1, 3, 5, 7, 20]\n",
    "\n",
    "bins=np.linspace(0, 1, num=51)\n",
    "fig, ax = plt.subplots()\n",
    "for navg in navgs:\n",
    "    y = (np.random.rand(npts, navg)**2).mean(axis=-1)\n",
    "    ax.hist(y, histtype='step', bins=bins, density=True, label=str(navg))\n",
    "ax.legend(loc='upper right')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how skewed the original distribution is.  As a result, more averaging is required to approach the Gaussian than was the case when we started with a uniform distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a distribution: calculate, then plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hist()` function in matplotlib is using `np.histogram` to do the calculation, so let's look at that underlying function.  Notice that it shares some argument names with `hist`.  Let's experiment with the `bins` argument and the `density` argument, using a uniform distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yu = np.random.rand(1000)\n",
    "\n",
    "for density in (False, True):\n",
    "    h, edges = np.histogram(yu, bins=[0, 0.5, 1], density=density)\n",
    "    print(\"density = %s: h is\" % density, h, 'edges is', edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yu = np.random.rand(1000)\n",
    "\n",
    "for density in (False, True):\n",
    "    h, edges = np.histogram(yu, bins=[0, 0.25, 1], density=density)\n",
    "    print(\"density = %s: h is\" % density, h, 'edges is', edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the count (first output, `density=False`) depends on the bin boundaries, but the density (PDF) does not, apart from fluctuations inherent in working with random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a cumulative distribution we need to take cumulative sum of the counts; for a CDF, we need a discrete integral of the density. We will use a few more bin boundaries, and keep them uneven, so we can see whether we are doing this correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 0.2, 0.3, 0.4, 0.6, 0.9, 1]\n",
    "\n",
    "h_counts, edges = np.histogram(yu, bins=bins, density=False)\n",
    "h_density, edges = np.histogram(yu, bins=bins, density=True)\n",
    "\n",
    "cumulative_counts = np.cumsum(h_counts)\n",
    "\n",
    "# Integrate the PDF:\n",
    "intervals = np.diff(bins)\n",
    "int_density = np.cumsum(h_density * intervals)\n",
    "\n",
    "print(\"Cumulative distribution:\", cumulative_counts)\n",
    "print(\"CDF:\", int_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculated this way, the last CDF value will always be 1.  The first value, corresponding to the left edge of the first bin, is 0, but it is not included in the output from the `histogram` function.  Below, we will prepend it to that output for plotting purposes, so we will have a value for each bin edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axc, axd) = plt.subplots(ncols=2)\n",
    "cc = np.hstack(([0], cumulative_counts))\n",
    "axc.plot(bins, cc, marker='o')\n",
    "axc.set_title('cumulative counts')\n",
    "\n",
    "cdf = np.hstack(([0], int_density))\n",
    "axd.plot(bins, cdf, marker='o')\n",
    "axd.set_title('CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you could also use the matplotlib `bar` function, or skip the explicit `np.histogram` calculation and let matplotlib do it for you via its `hist`, but for the cumulative distribution I think a line plot makes more sense.  Do you agree or disagree?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making your own (slow) histogram function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.histogram` function is fast and convenient, but to be sure you understand what it is doing your assignment is to write a version of your own, using a simple strategy of looping through the bins.  Here are some things you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have an array of bin boundaries, you can loop through the pairs of boundaries like this:\n",
    "bins = [1.1, 2.2, 3.3, 4.4, 5.5]\n",
    "print('method 1')\n",
    "for left, right in zip(bins[:-1], bins[1:]):\n",
    "    print(left, right)\n",
    "    \n",
    "# Or like this:\n",
    "print('\\nmethod 2')\n",
    "for i in range(len(bins)-1):\n",
    "    left, right = bins[i:i+2]\n",
    "    print(left, right)\n",
    "    \n",
    "# Or like this:\n",
    "print('\\nmethod3')\n",
    "for i in range(len(bins)-1):\n",
    "    left = bins[i]\n",
    "    right = bins[i+1]\n",
    "    print(left, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have a left and right, you will need to use them to select the appropriate values:\n",
    "xx = 10 * np.random.randn(500)\n",
    "left = 4.4\n",
    "right = 5.0\n",
    "selection = xx[(xx >= left) & (xx < right)]\n",
    "print(selection)\n",
    "print(\"We found %d values in the [%.2f, %.2f) bin\"\n",
    "      % (len(selection), left, right))  # or selection.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution-related functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has basic random number generation functions; scipy provides access to much more information about various theoretical distributions.  Here is a very brief example for the Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-4, 4, 500)\n",
    "\n",
    "fig, (axd, axc) = plt.subplots(ncols=2)\n",
    "\n",
    "axd.plot(x, ss.norm.pdf(x))\n",
    "axd.set_title('Normal PDF')\n",
    "\n",
    "axc.plot(x, ss.norm.cdf(x))\n",
    "axc.set_title('Normal CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `ss.norm?` to see the various functions (actually, methods) that are available.  Some of these will be very useful later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have a data set that appears to be approximately Gaussian.  How might you fit a Gaussian curve to it?  Let's try a simple approach.  We will start with a series that really is Gaussian.  We know the true mean and true standard deviation for our experiment, but of course with a real data set we would know only the sample mean and sample standard deviation.  We just use those as the parameters for our estimated Gaussian fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "ymean_true = 1.5\n",
    "ystd_true = 2.5\n",
    "y = ymean_true + ystd_true * np.random.randn(1000)\n",
    "ymean = y.mean()\n",
    "ystd = y.std()\n",
    "print('sample mean and standard deviation are', ymean, ystd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 13, 1000)\n",
    "y_pdf = ss.norm(loc=ymean, scale=ystd).pdf(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(y, 20, density=True)\n",
    "ax.plot(x, y_pdf, 'r', lw=2)\n",
    "ax.set_xlim(-10, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that even with 1000 independent points taken from a very good Gaussian distribution, the randomness means we don't get a perfect fit.  How would the fit change if we used more bins in the histogram?  Try it and see!  Make sure you understand, and can explain, how the fit of the histogram to the continuous pdf varies with the number of bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rayleigh distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are in a location where the vector-mean wind is zero, but the East and North components of the wind are approximately Gaussian, are uncorrelated, and have the same standard deviation--let's say it is 5 m/s.  The wind direction would be uniformly distributed around all points of the compass.  What would the distribution of the *speed* look like?  It certainly can't be Gaussian--it has to be positive. It should be a Rayleigh distribution.  Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "npts = 10000\n",
    "u = 5 * np.random.randn(npts)\n",
    "v = 5 * np.random.randn(npts)\n",
    "s = np.hypot(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 25, 1000)\n",
    "y = ss.rayleigh(scale=5).pdf(x)\n",
    "\n",
    "nbins=20\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(s, nbins, density=True);\n",
    "ax.plot(x, y, 'r', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that although the Gaussian distribution has two free parameters, the Rayleigh distribution has only one: the scale.  How can we estimate it?  It is not the mean or the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean and median of the speed are\", s.mean(), np.median(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scale parameter--which gives the location of the *mode*, or highest point on the pdf-- is $\\sqrt{\\frac{2}{\\pi}}$ times the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"estimated mode is\", np.sqrt(2/np.pi) * s.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
