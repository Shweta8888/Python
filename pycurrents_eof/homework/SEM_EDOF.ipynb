{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with correlation using fake time series.  We will also take a quick look at interpolation.  In all of the following, the assumption is that arrays contain data sampled uniformly in time or space.  (This can be relaxed when calculating the correlation coefficient at zero lag.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Our standard imports:\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access to many standard distributions:\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is find out what functions are available, and how to use at least some of them. There are several types of calculation in the category of \"correlation\".  \"Autocorrelation\" is the correlation of a time series with a lagged copy of itself.  \"Crosscorrelation\" is correlation between two series of the same length, with or without lags. \"Correlation coefficient\" is a normalized correlation.  \"Convolution\" is a lagged correlation in which the series may differ in length, and in which one of the series is reversed.  There are also variations involving normalization, and the treatment of complex numbers.  The terminology is not entirely standardized, and can differ from one discipline to another. Before you use any given function, check to be sure you know exactly what it is doing. I suggest that you take a look at \n",
    "\n",
    "- http://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html#numpy.corrcoef\n",
    "- http://docs.scipy.org/doc/numpy/reference/generated/numpy.ma.corrcoef.html#numpy.ma.corrcoef (for masked arrays)\n",
    "- http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html#numpy.correlate\n",
    "- http://docs.scipy.org/doc/numpy/reference/generated/numpy.convolve.html#numpy.convolve\n",
    "- http://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.fftconvolve.html#scipy.signal.fftconvolve\n",
    "\n",
    "Notice that only one of these handles masked arrays; it calculates the correlation coefficients with zero lag.  For anything else, if you have masked arrays or NaNs, you need to write your own code to handle the missing points appropriately.  Unless you are calculating a zero-lag correlation, deleting missing points, thereby shortening an array, will *not* be a good solution.  You will need to replace the bad points via interpolation or some other method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with the correlation coefficient between two independent random series. We will make an ensemble of series, and find the PDF of the correlation coefficient as the normalized histogram of correlation coefficients for all pairs. The `np.corrcoef` function provides an efficient way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrials = 200\n",
    "npts = 100\n",
    "\n",
    "np.random.seed(1234)\n",
    "y = np.random.rand(ntrials, npts)  # uniform distribution\n",
    "ccoef = np.corrcoef(y)\n",
    "print('The shape of y is %s, the shape of ccoef is %s.' \n",
    "      % (y.shape, ccoef.shape))\n",
    "\n",
    "# we want all of the values except for the diagonal\n",
    "di = np.diag_indices(ntrials)\n",
    "ccoef = np.ma.asarray(ccoef)\n",
    "ccoef[di] = np.ma.masked\n",
    "ccoef_flat = ccoef.compressed()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(ccoef_flat, bins=np.arange(-0.500, 0.5001, 0.01),\n",
    "        density=True)\n",
    "ax.grid(True)\n",
    "ax.set_ylabel('PDF')\n",
    "ax.set_xlabel('Correlation coefficient')\n",
    "ax.set_title('Uniform distribution, %d points' % npts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try the same thing with a normal distribution, or any other variation you like.  What happens if you use a small number of points in the series, say 10, instead of the larger number (100) in the example above?  Think about it, then experiment!\n",
    "\n",
    "We can calculate the correlation coefficient using other functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "# With correlate:\n",
    "# We must remove the means.\n",
    "cc1 = np.correlate(x - x.mean(), y - y.mean())[0]\n",
    "# And we must normalize by the number of points\n",
    "# and the product of the standard deviations.\n",
    "cc1 /= (len(x) * x.std() * y.std())\n",
    "\n",
    "# with corrcoef:\n",
    "cc2 = np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "print(cc1, cc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `correlate` function is just returning a *sum of products*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `correlate` function is more general, however; with the `mode='full'` kwarg, it is useful for calculating the correlation as a function of lag.  Start with autocorrelation. Note that zero-padding is used, so that the estimated sample correlation at a given lag is the sum of the overlapping products.  To get an estimate of the autocorrelation function, we normalize by the value at zero-lag.  The zero-padding builds in a taper towards zero at maximum magnitude of the lag.  Hence it is a biased estimator, but one with reduced mean squared error compared to an unbiased estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 50 \n",
    "x = np.random.randn(nx) # normal RV\n",
    "\n",
    "lags = np.arange(-nx + 1, nx) # so last value is nx - 1\n",
    "\n",
    "# Remove sample mean.\n",
    "xdm = x - x.mean()\n",
    "\n",
    "autocorr_xdm = np.correlate(xdm, xdm, mode='full')\n",
    "# Normalize by the zero-lag value:\n",
    "autocorr_xdm /= autocorr_xdm[nx - 1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lags, autocorr_xdm, 'r')\n",
    "ax.set_xlabel('lag')\n",
    "ax.set_ylabel('correlation coefficient')\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the example above, removing the sample mean has very little effect, because the sample mean is nearly zero.  With typical data sets, this will not be the case. In general you *should* remove the sample mean before estimating the autocorrelation--and the `correlate` function does not do this for you. \n",
    "\n",
    "\n",
    "\n",
    "Exercise: illustrate this by executing the above cell with `randn` changed to `rand`.\n",
    "\n",
    "\n",
    "\n",
    "Exercise: execute the cell multiple times, noting that the plot changes each time, because each call to `randn` yields a different set of numbers.  The true autocorrelation is zero at all non-zero lags, but the *sample* autocorrelation estimate is not zero.\n",
    "\n",
    "\n",
    "\n",
    "Exercise: execute the above cell with different values of nx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentially correlated data: equivalent degrees of freedom (EDOF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make a function to calculate the estimated autocorrelation, so we don't have to keep typing in the same steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr(x, twosided=False, tapered=True):\n",
    "    \"\"\"\n",
    "    Return (lags, ac), where ac is the estimated autocorrelation \n",
    "    function for x, at the full set of possible lags.\n",
    "    \n",
    "    If twosided is True, all lags will be included;\n",
    "    otherwise (default), only non-negative lags will be included.\n",
    "\n",
    "    If tapered is True (default), the low-MSE estimate, linearly\n",
    "    tapered to zero for large lags, is returned.\n",
    "    \"\"\"\n",
    "    nx = len(x)\n",
    "    xdm = x - x.mean()\n",
    "    ac = np.correlate(xdm, xdm, mode='full')\n",
    "    ac /= ac[nx - 1]\n",
    "    lags = np.arange(-nx + 1, nx)\n",
    "    if not tapered:  # undo the built-in taper\n",
    "        taper = 1 - np.abs(lags) / float(nx)\n",
    "        ac /= taper\n",
    "    if twosided:\n",
    "        return lags, ac\n",
    "    else:\n",
    "        return lags[nx-1:], ac[nx-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we also need a function for estimating the equivalent number of degrees of freedom (EDOF) for the standard error of the mean.  Note that the EDOF is itself just an uncertain estimate, specific to a particular statistic (here, to the SEM), and with no one perfect estimator.\n",
    "\n",
    "Let's just make a single function that returns the mean, the SEM, and the EDOF used to estimate the SEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sem_edof(y, truncated=True, tapered_cor=True):\n",
    "    \"\"\"\n",
    "    Return the mean, SEM, and EDOF for the sequence y.\n",
    "\n",
    "    If truncated is True (default), the EDOF and SEM will\n",
    "    be calculated based on only the positive central peak of\n",
    "    the sample autocorrelation.\n",
    "\n",
    "    If tapered_cor is True (default), the low-MSE estimate of\n",
    "    the lagged correlation is used.\n",
    "    \"\"\"\n",
    "    ym = y.mean()\n",
    "    n = len(y)\n",
    "    lags, ac = autocorr(y, twosided=True, tapered=tapered_cor)\n",
    "    taper = 1 - np.abs(lags) / n\n",
    "    if truncated:\n",
    "        i1 = np.nonzero(np.logical_and(lags >= 0, ac < 0))[0].min()\n",
    "        i0 = 2 * n - i1 - 1  # taking advantage of symmetry...\n",
    "        sl = slice(i0, i1)\n",
    "    else:\n",
    "        sl = slice(None)\n",
    "    edof = n / np.sum(taper[sl] * ac[sl])\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        sem = y.std() / np.sqrt(edof)\n",
    "    return ym, sem, edof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a set of uncorrelated points and apply a smoothing operator, the output will be sequentially correlated. This is illustrated below with a 5-point boxcar filter, implemented using the `convolve` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 200\n",
    "t = np.arange(nx)\n",
    "x = np.random.randn(nx)\n",
    "xc = np.convolve(x, np.ones(5) / 5.0, mode='same')\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(2)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "ax0.plot(t, x, 'b', t, xc, 'r')\n",
    "ax0.set_xlabel('time')\n",
    "ax0.set_ylabel('random data')\n",
    "\n",
    "lags, auto_x = autocorr(x)\n",
    "lags, auto_xc = autocorr(xc)\n",
    "ax1.plot(lags, auto_x, 'b', lags, auto_xc, 'r')\n",
    "ax1.set_xlabel('lag')\n",
    "ax1.set_ylabel('correlation')\n",
    "\n",
    "for ax in (ax0, ax1):\n",
    "    ax.locator_params(axis='y', nbins=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how smoothing a series of independent random numbers has yielded a slowly-varying time series, and an estimated autocorrelation function with a wider central lobe and larger values in the side lobes where the *true* autocorrelation is zero; values separated by more than the width of the smoothing window are actually still statistically independent, but this is not clear based on the *sample* autocorrelation.\n",
    "\n",
    "\n",
    "\n",
    "Exercise: use simple statistical concepts to explain *why* we see these *larger* sidelobes.\n",
    "\n",
    "\n",
    "\n",
    "Exercise: calculate mean, SEM, and EDOF for many realizations, and quantify how well or how badly this estimate of SEM matches the actual statistics of the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for truncated in (True, False):\n",
    "    print(\"Integrating over central peak? \", truncated)\n",
    "    for tapered_cor in (True, False):\n",
    "        print(\"  Tapered correlation estimate? \", tapered_cor)\n",
    "        print(\"    x:  %7.3f  %7.3f  %9.1f \" % \n",
    "              mean_sem_edof(x, truncated=truncated, tapered_cor=tapered_cor))\n",
    "        print(\"    xc: %7.3f  %7.3f  %9.1f \" % \n",
    "              mean_sem_edof(xc, truncated=truncated, tapered_cor=tapered_cor)) \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that integrating over the whole lagged autocorrelation function fails if the tapered estimator is not used, and is wildly inaccurate even when the tapered estimator is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What one sees most often in data analysis papers is not the autocorrelation, but the cross correlation between two time series, or between a single time series, such as the Southern Oscillation Index (SOI) and the time series at each of many locations, such as SST on a 1-degree grid.  Such cross correlations can be at zero lag, or as a function of lag.  The calculation is straightforward; the main point of confusion is the definition of the lag.  Let's experiment with two time series, each with a sinusoid plus some noise.  The first, `y1`, reaches its maximum value 1/4 cycle *after* the second one, `y2`.  Therefore `y1` *lags* `y2`, or `y2` *leads* `y1`.  Unfortunately, there is no single convention for defining the cross-covariance or cross-correlation; Jenkins and Watts (1968) use one convention, and Emery and Thompson (2001) use the opposite.  The `np.correlate()` function matches Emery and Thompson; we will use that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 500\n",
    "x = np.linspace(0, 50, npts)\n",
    "y1 = 5 * np.sin(x/2) + np.random.randn(npts)\n",
    "y2 = 5 * np.cos(x/2) + np.random.randn(npts)\n",
    "\n",
    "lags = np.arange(-npts + 1, npts)\n",
    "ccov = np.correlate(y1 - y1.mean(), y2 - y2.mean(), mode='full')\n",
    "ccor = ccov / (npts * y1.std() * y2.std())\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "ax = axs[0]\n",
    "ax.plot(x, y1, 'b', label='y1')\n",
    "ax.plot(x, y2, 'r', label='y2')\n",
    "ax.set_ylim(-10, 10)\n",
    "ax.legend(loc='upper right', fontsize='small', ncol=2)\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(lags, ccor)\n",
    "ax.set_ylim(-1.1, 1.1)\n",
    "ax.set_ylabel('cross-correlation')\n",
    "ax.set_xlabel('lag of y1 relative to y2')\n",
    "\n",
    "maxlag = lags[np.argmax(ccor)]\n",
    "print(\"max correlation is at lag %d\" % maxlag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can all get quite confusing.  The red curve, `y2`, peaks to the *left* of `y1`.  Doesn't that mean it *lags* `y1`?  No, it *leads*.  Think of yourself moving forward in time, to the right along the x-axis.  As you slide along, you see the red curve peak *before* the blue.\n",
    "\n",
    "To summarize: with the calculation done as above, a positive lag means the first series lags the second, or the second leads the first--peaks earlier in time, so at a location to the left on the time series plot.\n",
    "\n",
    "You will also have noticed that this estimator of the cross-correlation decays to zero at the ends.  This is a biased estimator.  We could remove that bias by normalizing by the number of points going into the sum instead of by the total number of points.  In practice it usually doesn't matter.  To get a reliable lagged correlation, one needs a large number of points, and usually the only lags of interest are a small fraction of the total number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irregular sampling and interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation of statistical constructs such as the lagged autocorrelation function are vastly simpler when the data are uniformly spaced.  When real data are not uniformly sampled, we often use the expedient of interpolation to put them on a uniform grid for analysis.  The simplest method, linear interpolation, is usually good enough; and a general principle of data analysis is, \"use the simplest technique that will reveal the signal of interest\".  (Actually, there is a simpler method than linear interpolation.  If the sampling is only slightly non-uniform, so that sample times are close to regular grid points, then nearest-neighbor interpolation to the uniform grid can be superior to interpolation.)\n",
    "\n",
    "\n",
    "\n",
    "Here, we will make a quite irregular set of fake sample times by summing uniformly distributed random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 50\n",
    "t = 2 * np.random.rand(50).cumsum()\n",
    "x = np.random.randn(nt)\n",
    "\n",
    "# Make a uniform grid, with the same number of points:\n",
    "tnew = np.linspace(t.min(), t.max(), nt)\n",
    "xnew = np.interp(tnew, t, x)\n",
    "\n",
    "print(\"standard deviations: \", x.std(), xnew.std())\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(2)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "ax0.plot(t, x, 'bo', label='original') \n",
    "ax0.plot(tnew, xnew, 'ro', label='interpolated')\n",
    "ax0.set_xlabel('time')\n",
    "ax0.set_ylabel('measurement')\n",
    "ax0.legend(loc='upper right', ncol=2, fontsize='small')\n",
    "ax0.set_ylim(-5, 5)\n",
    "\n",
    "lags, auto_x = autocorr(xnew)\n",
    "ax1.plot(lags, auto_x, 'ro')\n",
    "\n",
    "for ax in (ax0, ax1):\n",
    "    ax.locator_params(axis='y', nbins=4)\n",
    "\n",
    "ax1.set_xlim(0, nt//3) # To see the smaller lags better.\n",
    "ax1.set_ylim(-1.1, 1.1)\n",
    "ax1.set_xlabel('lag')\n",
    "ax1.set_ylabel('autocorrelation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the interpolation has reduced the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolating gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often observations are taken at uniform intervals, but there are gaps because something went wrong. We will illustrate with a fake time series consisting of a sinusoid plus noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 500\n",
    "x = np.linspace(0, 50, npts)\n",
    "y = 5 * np.sin(x/2) + np.random.randn(npts)\n",
    "y_orig = y.copy() \n",
    "\n",
    "# Insert some nans:\n",
    "y[::7] = np.nan\n",
    "y[::10] = np.nan\n",
    "\n",
    "ym = np.ma.masked_invalid(y)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, ym, 'bo', label='retained')\n",
    "ax.plot(x[ym.mask], y_orig[ym.mask], 'ro', label='deleted')\n",
    "ax.set_ylim([-10, 10])\n",
    "ax.legend(loc='upper right', fontsize='small', ncol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now interpolate.  We need to make x and y ndarrays with no gaps by removing the points where ym is masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab = np.ma.array(x, mask=ym.mask).compressed()\n",
    "ytab = ym.compressed()\n",
    "\n",
    "yfilled = ym.filled()   # no longer masked\n",
    "yfilled[ym.mask] = np.interp(x[ym.mask], xtab, ytab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, ym, 'bo', label='retained')\n",
    "ax.plot(x[ym.mask], yfilled[ym.mask], 'ro', label='interpolated')\n",
    "ax.set_ylim([-10, 10])\n",
    "ax.legend(loc='upper right', fontsize='small', ncol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like using masked arrays, but of course we could easily do the same thing without detouring through ma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.isnan(y)\n",
    "yfilled2 = y.copy()\n",
    "yfilled2[mask] = np.interp(x[mask], x[~mask], y[~mask])\n",
    "\n",
    "identical = (yfilled2 == yfilled).all()\n",
    "print(\"They are identical?\", identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
