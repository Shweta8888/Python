{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Orthogonal Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simple example to illustrate how EOFs are related to the data from which they are calculated, and to show two ways of calculating them.  The second method, using the Singular Value Decomposition, is usually preferred in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a fake dataset with 4 spatial points. Note that we are arranging our array with time as the first dimension, increasing with row number, so each column is a time series.  We could use any number of spatial locations (number of columns), and they don't have to be arranged any particular way.  They could be all points in a rectangular lon-lat grid, for example, strung out out into a single list of points by flattening the 2-D array.  To begin with, it is easiest to see how the calculation works by keeping the number of spatial points very small, hence our choice of 4 points; but in general there can even be more spatial points than temporal samples, and in fact there is nothing special about \"space\" and \"time\", or the choice of which one is the first dimension and which is the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 4\n",
    "nt = 100\n",
    "t = np.linspace(0, 1, nt)\n",
    "dat = np.zeros((nt, nx), dtype=float)\n",
    "y1 = np.sin(t * (2 * np.pi / 0.11))\n",
    "y2 = np.sin(t * (2 * np.pi / 0.31))\n",
    "dat[:,0] = y1\n",
    "dat[:,1] = 0.5 * y1 + 0.6 * y2\n",
    "dat[:,2] = 0.25 * y1 + 0.8 * y2\n",
    "dat[:,3] = y2\n",
    "# Save a copy of the pure signal.\n",
    "signaldat = dat.copy()\n",
    "\n",
    "#add noise\n",
    "noisefac = 0.15\n",
    "np.random.seed(0) # make the \"random\" numbers repeatable\n",
    "dat += noisefac * np.random.randn(nt, nx)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, dat) # plot the 4 time series (columns)\n",
    "ax.set_title(\"Fake time series from 4 locations\")\n",
    "ax.legend([\"0\", \"1\", \"2\", \"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have designed our fake data so that there should be correlation through `y1` among the first three columns, and through 'y2' among the last three.  Although there are 4 columns, there are really only 2 signals--that is, temporal patterns that appear in more than one column. This is hardly obvious from the plot of the columns, though; it all looks rather messy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOFs calculated using the eigenvalues and eigenvectors of the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the eigenvalues and eigenvectors from the covariance matrix, or from that matrix times the number of points--in other words, from the summed products rather than from the mean products.  You can verify that the end result is the same, apart from a scale factor of `nt` in the eigenvalues.  We will use the summed products because it will make the eigenvalues match those calculated using the SVD method below, and it corresponds nicely to the usual linear algebra treatment of the problem.\n",
    "\n",
    "Typically one wants to remove the time mean at each spatial point, so that the dataset is separated into a time-mean and the variation about that mean.  We will do that here.\n",
    "\n",
    "If our de-meaned data matrix is $A$, then `nt` times the covariance matrix is $A' A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datmean = dat.mean(axis=0) # time mean\n",
    "dat_dm = dat - datmean # numpy broadcasting at work...\n",
    "\n",
    "covdat = (dat_dm.T @ dat_dm) / nt # matrix multiplication \n",
    "# this is the same as nt * np.cov(dat, bias=True)\n",
    "vals, vecs = np.linalg.eig(covdat)\n",
    "print(\"Covariance matrix is:\\n\", covdat)\n",
    "print(\"Eigenvalues are:\\n\", vals)\n",
    "print(\"Eigenvectors are columns of:\\n\", vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvalues will be positive, but are not necessarily in any particular order; let's sort the eigenvalues and eigenvectors in descending order. The `argsort()` function returns the indices that sort them in ascending order, and then we use `[::-1]` indexing to reverse the order. \n",
    "\n",
    "(To be clear: in this particular example the eigenvalues happen to have come out in sorted order, but you can't count on it.  That's why we are demonstrating how to sort them.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isort = np.argsort(vals)[::-1]\n",
    "vals = vals[isort]\n",
    "vecs = vecs[:, isort] # re-arrange the columns\n",
    "print(\"Eigenvalues are:\\n\", vals)\n",
    "print(\"Eigenvectors are columns of:\\n\", vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can project the original de-meaned data onto the new set of spatial basis functions to get the time series corresponding to each spatial eigenvector. This projection is a rotation, so the time mean of each series after rotation is still zero, within the limits of numerical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvecs = dat_dm @ vecs  # projection on new basis\n",
    "print(\"time means after projection:\\n\", tvecs.mean(axis=0))\n",
    "print(\"are all very small.\")\n",
    "plt.figure()\n",
    "plt.plot(t, tvecs)\n",
    "plt.legend([\"0\", \"1\", \"2\", \"3\"])\n",
    "plt.title(\"EOF temporal functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, verify that the eigenvalues are `nt` times the variances of the temporal expansions. (By default, `var()` provides the sample variance, dividing by `nt`, not `nt - 1`.)  If we had used the covariance, the eigenvalues would have been the variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variance:\\n\", tvecs.var(axis=0))  # along time axis\n",
    "print(\"Eigenvalues:\\n\", vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first two eigenvalues are much larger than the other two, and that you can see the periodicity predominantly of y2 in the first one, of y1 in the second one, and nothing more than noise in the other two.  \n",
    "\n",
    "We might have hoped that the EOF decomposition would more perfectly separate y2 from y1, but it can't magically do this.  The EOFs are constructed simply as the orthogonal basis that diagonalizes the covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reconstruct the original data array by summing the EOFs and adding back the time mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_from_eofs = tvecs @ vecs.T + datmean\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, dat_from_eofs)\n",
    "ax.set_prop_cycle(None) # Resets the color cycle.\n",
    "ax.plot(t, dat, '.')\n",
    "ax.set_title(\"Reconstructed; original (dotted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the round-trip, from `dat_dm` to `tvec` and back to `dat_dm` involved first right-multiplication of `dat_dm` by `vecs` and then right-multiplication of that by `vecs.T`.  Equivalently, we could have first done a left-multiplication of `dat_dm` by `vecs.T`, followed by left-multiplication by `vecs`.  In either case, the round-trip involves the product of $X X'$ or its transpose, where $X$ is `vecs`.  Because $X$ is orthonormal, $X X'$ is the identity matrix--hence the round-trip. Using the normal matrix notation, the data matrix, $A$, is `dat_dm`, with each column being a time series at a given spatial location.  Then $A X$ is a matrix with columns that are linear combinations of the columns of `dat_dm`; each row is a projection of the original row onto a rotated coordinate system in which the first axis is oriented in the direction of greatest variance, and so on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we sum only the first two EOFs, the ones with most of the variance and with structure that looks like signal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_from_2eofs = tvecs[:,:2] @ vecs.T[:2]\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, \n",
    "                                    sharex=True,\n",
    "                                    sharey=True,\n",
    "                                    figsize=(6,10))\n",
    "ax0.plot(t, dat_from_eofs - datmean)\n",
    "ax0.set_title('From all EOFs')\n",
    "ax1.plot(t, dat_from_2eofs)\n",
    "ax1.set_title('From first 2 EOFs')\n",
    "ax2.plot(t, signaldat - signaldat.mean(axis=0))\n",
    "ax2.set_title('De-meaned signal (no noise)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that throwing away the last 2 EOFs has reduced the noise a little bit, giving a better approximation to the pure signal.  This noise reduction would be more effective if we had more spatial points, such as with basin-scale maps of SSH or SST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EOFs using Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to calculate EOFs is to use the Singular Value Decomposition (SVD), which essentially does the whole calculation in one step.  One has to be a little bit careful about precisely how the svd function is implemented--mainly about what it returns.\n",
    "\n",
    "The basic idea of SVD is that any matrix can be factored as the product of three other matrices, the middle one of which is diagonal, with the other two being orthonormal.  Again let our `dat_dm` be $A$, with dimensions `(nt, nx)`.  Then $A = U D V'$, where the columns of $U$ are the eigenvectors of $A A'$ and the columns of $V$ are the eigenvectors of $A' A$.  $A A'$ and $A' A$ have the same nonzero eigenvalues, which are the square of the nonzero diagonal elements of $D$.\n",
    "\n",
    "In this formulation, $U$ is `(nt, nt)`, $D$ is `(nt, nx)`, and $V$ is `(nx, nx)`, but the number of non-zero eigenvalues is the lesser of `nt` and `nx`; let's call it `ne`.  Then, to omit the columns corresponding to zero eigenvalues, the dimensions of $U$ can be `(nt, ne)`, of $D$ can be `(ne, ne)`, and of $V'$ can be `(ne, nx)`.\n",
    "\n",
    "Implementations of SVD can return full-sized or reduced matrices; and $D$ can be returned as a matrix, or as a 1-D array with just the diagonal values.  We will use `np.linalg.svd` with the `full_matrices=False` keyword argument; consequently we will get the reduced arrays and just the 1-D array of eigenvalues.  In addition, note that this returns $U$, the diagonal elements of $D$, and $V'$ (already transposed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, d, vt = np.linalg.svd(dat_dm, full_matrices=False)\n",
    "\n",
    "print(\"Eigenvalues from A'A:\\n\", vals)\n",
    "print(\"Eigenvalues from SVD:\\n\", (d ** 2) / nt)  # Note the power of 2.\n",
    "\n",
    "vecs_svd = vt.T\n",
    "print(\"\\nEigenvectors from A'A (columns):\\n\", vecs)\n",
    "print(\"Eigenvectors from SVD (columns):\\n\", vecs_svd)\n",
    "\n",
    "tvecs_svd = np.dot(u, np.diag(d))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t, tvecs_svd)\n",
    "plt.legend([\"0\", \"1\", \"2\", \"3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Eigenvectors calculated using the two different methods are the same *except* for possible sign reversals.  The product of any scalar and an eigenvector still satisfies the eigenvector equation for the same eigenvalue, so the overall sign is arbitrary. It is a convention that eigenvectors are normalized to unit length, but there is no convention for determining the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
